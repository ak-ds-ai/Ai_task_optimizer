{% extends "base.html" %}

{% block title %}Documentation - EmotionAI System Guide{% endblock %}

{% block extra_css %}
<style>
    .doc-hero {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 3rem 2rem;
        text-align: center;
    }

    .doc-container {
        max-width: 1200px;
        margin: 0 auto;
        padding: 3rem 2rem;
        display: grid;
        grid-template-columns: 250px 1fr;
        gap: 3rem;
    }

    /* Sidebar Navigation */
    .doc-sidebar {
        position: sticky;
        top: 90px;
        height: fit-content;
    }

    .doc-nav {
        background: white;
        border-radius: 15px;
        padding: 1.5rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    .doc-nav h3 {
        color: #667eea;
        margin-bottom: 1rem;
        font-size: 1.1rem;
    }

    .doc-nav ul {
        list-style: none;
    }

    .doc-nav li {
        margin-bottom: 0.5rem;
    }

    .doc-nav a {
        text-decoration: none;
        color: #7f8c8d;
        display: block;
        padding: 0.5rem;
        border-radius: 5px;
        transition: all 0.2s;
    }

    .doc-nav a:hover,
    .doc-nav a.active {
        background: rgba(102, 126, 234, 0.1);
        color: #667eea;
        padding-left: 1rem;
    }

    /* Content Area */
    .doc-content {
        background: white;
        border-radius: 15px;
        padding: 3rem;
        box-shadow: 0 4px 6px rgba(0,0,0,0.1);
    }

    .doc-section {
        margin-bottom: 3rem;
        padding-top: 2rem;
        border-top: 2px solid #ecf0f1;
    }

    .doc-section:first-child {
        border-top: none;
        padding-top: 0;
    }

    .doc-section h2 {
        color: #2c3e50;
        margin-bottom: 1rem;
        font-size: 2rem;
    }

    .doc-section h3 {
        color: #667eea;
        margin: 1.5rem 0 1rem;
        font-size: 1.5rem;
    }

    .doc-section h4 {
        color: #7f8c8d;
        margin: 1rem 0 0.5rem;
        font-size: 1.2rem;
    }

    .doc-section p {
        color: #7f8c8d;
        line-height: 1.8;
        margin-bottom: 1rem;
    }

    .doc-section ul,
    .doc-section ol {
        color: #7f8c8d;
        line-height: 1.8;
        margin-left: 2rem;
        margin-bottom: 1rem;
    }

    /* Code Blocks */
    .code-block {
        background: #2c3e50;
        color: #ecf0f1;
        padding: 1.5rem;
        border-radius: 10px;
        overflow-x: auto;
        margin: 1rem 0;
        position: relative;
    }

    .code-block code {
        font-family: 'Courier New', monospace;
        font-size: 0.9rem;
        line-height: 1.6;
    }

    .code-header {
        background: #34495e;
        color: white;
        padding: 0.5rem 1rem;
        border-radius: 10px 10px 0 0;
        font-size: 0.85rem;
        font-weight: 600;
        margin-bottom: -1rem;
    }

    /* Info Boxes */
    .info-box {
        padding: 1.5rem;
        border-radius: 10px;
        margin: 1.5rem 0;
        border-left: 4px solid;
    }

    .info-box.note {
        background: rgba(102, 126, 234, 0.1);
        border-color: #667eea;
    }

    .info-box.warning {
        background: rgba(255, 193, 7, 0.1);
        border-color: #ffc107;
    }

    .info-box.tip {
        background: rgba(56, 239, 125, 0.1);
        border-color: #38ef7d;
    }

    .info-box-title {
        font-weight: 600;
        margin-bottom: 0.5rem;
        display: flex;
        align-items: center;
        gap: 0.5rem;
    }

    /* Tables */
    .doc-table {
        width: 100%;
        border-collapse: collapse;
        margin: 1.5rem 0;
    }

    .doc-table th {
        background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
        color: white;
        padding: 1rem;
        text-align: left;
    }

    .doc-table td {
        padding: 1rem;
        border-bottom: 1px solid #ecf0f1;
    }

    .doc-table tr:hover {
        background: rgba(102, 126, 234, 0.05);
    }

    /* API Response */
    .api-response {
        background: #f8f9fa;
        padding: 1rem;
        border-radius: 8px;
        border-left: 4px solid #667eea;
        margin: 1rem 0;
    }

    @media (max-width: 968px) {
        .doc-container {
            grid-template-columns: 1fr;
        }

        .doc-sidebar {
            position: static;
        }

        .doc-content {
            padding: 2rem;
        }
    }
</style>
{% endblock %}

{% block content %}
<section class="doc-hero">
    <h1>üìö Documentation</h1>
    <p>Complete guide to using the EmotionAI Multi-Modal Analysis System</p>
</section>

<div class="doc-container">
    <!-- Sidebar Navigation -->
    <aside class="doc-sidebar">
        <nav class="doc-nav">
            <h3>Contents</h3>
            <ul>
                <li><a href="#overview" class="active">System Overview</a></li>
                <li><a href="#architecture">Architecture</a></li>
                <li><a href="#installation">Installation</a></li>
                <li><a href="#usage">How to Use</a></li>
                <li><a href="#api">API Reference</a></li>
                <li><a href="#models">Model Details</a></li>
                <li><a href="#examples">Code Examples</a></li>
                <li><a href="#troubleshooting">Troubleshooting</a></li>
            </ul>
        </nav>
    </aside>

    <!-- Main Content -->
    <main class="doc-content">
        <!-- Overview -->
        <section class="doc-section" id="overview">
            <h2>üìã System Overview</h2>
            <p>
                EmotionAI is an advanced multi-modal emotion analysis system that combines 
                state-of-the-art natural language processing and computer vision techniques 
                to detect and analyze human emotions with high accuracy.
            </p>

            <h3>Key Features</h3>
            <ul>
                <li><strong>Multi-Modal Analysis:</strong> Combines text and facial emotion detection</li>
                <li><strong>28 Text Emotions:</strong> Powered by Google's GoEmotions dataset</li>
                <li><strong>7 Facial Emotions:</strong> Using FER-2013 trained models</li>
                <li><strong>95%+ Accuracy:</strong> When both modalities are available</li>
                <li><strong>Real-time Processing:</strong> <200ms response time with GPU</li>
                <li><strong>Smart Recommendations:</strong> Context-aware task suggestions</li>
                <li><strong>HR Alert System:</strong> Automatic notifications for wellbeing</li>
            </ul>

            <div class="info-box note">
                <div class="info-box-title">üí° Note</div>
                <p>This system is designed for workplace wellness monitoring, customer sentiment analysis, 
                and educational research. Always obtain proper consent before analyzing emotions.</p>
            </div>
        </section>

        <!-- Architecture -->
        <section class="doc-section" id="architecture">
            <h2>üèóÔ∏è System Architecture</h2>
            
            <h3>Technology Stack</h3>
            <table class="doc-table">
                <thead>
                    <tr>
                        <th>Component</th>
                        <th>Technology</th>
                        <th>Purpose</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td><strong>Backend</strong></td>
                        <td>Flask (Python 3.10+)</td>
                        <td>Web framework and API</td>
                    </tr>
                    <tr>
                        <td><strong>NLP Model</strong></td>
                        <td>RoBERTa (GoEmotions)</td>
                        <td>Text emotion analysis</td>
                    </tr>
                    <tr>
                        <td><strong>CV Model</strong></td>
                        <td>DeepFace (FER-2013)</td>
                        <td>Facial emotion recognition</td>
                    </tr>
                    <tr>
                        <td><strong>Deep Learning</strong></td>
                        <td>PyTorch + TensorFlow</td>
                        <td>Model inference</td>
                    </tr>
                    <tr>
                        <td><strong>Database</strong></td>
                        <td>SQLite</td>
                        <td>Emotion logging</td>
                    </tr>
                </tbody>
            </table>

            <h3>Data Flow</h3>
            <div class="code-block">
                <code>User Input (Text/Image)
    ‚Üì
Input Processing & Validation
    ‚Üì
Parallel Analysis
    ‚îú‚îÄ‚îÄ Text ‚Üí RoBERTa ‚Üí 28 emotions
    ‚îî‚îÄ‚îÄ Image ‚Üí DeepFace ‚Üí 7 emotions
    ‚Üì
Multi-Modal Fusion (Weighted Average)
    ‚Üì
Final Emotion Detection
    ‚Üì
Task Recommendation Engine
    ‚Üì
HR Alert System (if needed)
    ‚Üì
Response & Analytics Storage</code>
            </div>
        </section>

        <!-- Installation -->
        <section class="doc-section" id="installation">
            <h2>üíª Installation</h2>

            <h3>Prerequisites</h3>
            <ul>
                <li>Python 3.8 or higher</li>
                <li>pip package manager</li>
                <li>4GB RAM minimum (8GB recommended)</li>
                <li>3GB free disk space for models</li>
            </ul>

            <h3>Step 1: Clone or Create Project</h3>
            <div class="code-header">Terminal</div>
            <div class="code-block">
                <code>mkdir emotion_ai_project
cd emotion_ai_project
mkdir templates uploads static</code>
            </div>

            <h3>Step 2: Create Virtual Environment</h3>
            <div class="code-header">Terminal</div>
            <div class="code-block">
                <code># Windows
python -m venv venv
venv\Scripts\activate

# Mac/Linux
python3 -m venv venv
source venv/bin/activate</code>
            </div>

            <h3>Step 3: Install Dependencies</h3>
            <div class="code-header">requirements.txt</div>
            <div class="code-block">
                <code>Flask==3.0.0
Flask-CORS==4.0.0
deepface==0.0.89
opencv-python==4.8.1.78
transformers==4.35.2
torch==2.1.1
tensorflow==2.17.0
numpy==1.24.3
Pillow==10.1.0</code>
            </div>

            <div class="code-header">Terminal</div>
            <div class="code-block">
                <code>pip install -r requirements.txt</code>
            </div>

            <div class="info-box warning">
                <div class="info-box-title">‚ö†Ô∏è Important</div>
                <p>First run will download ~700MB of AI models (GoEmotions + DeepFace). 
                This is a one-time download and may take 3-5 minutes.</p>
            </div>

            <h3>Step 4: Run Application</h3>
            <div class="code-header">Terminal</div>
            <div class="code-block">
                <code>python app.py

# Visit: http://localhost:5000</code>
            </div>
        </section>

        <!-- Usage -->
        <section class="doc-section" id="usage">
            <h2>üéØ How to Use</h2>

            <h3>1. Text-Only Analysis</h3>
            <p>Navigate to the Demo page and enter your text in the text input field:</p>
            <div class="code-header">Example Input</div>
            <div class="code-block">
                <code>"I'm feeling stressed about the upcoming deadline but excited about the project!"</code>
            </div>
            <p>Click "Analyze Text" to get emotion breakdown and recommendations.</p>

            <h3>2. Facial-Only Analysis</h3>
            <p>You can either:</p>
            <ul>
                <li>Click "Start Camera" and capture a photo from your webcam</li>
                <li>Drag and drop an image file</li>
                <li>Click the upload area to select a file</li>
            </ul>
            <p>Then click "Analyze Emotion" to process.</p>

            <h3>3. Multi-Modal Analysis</h3>
            <p>For the most accurate results, provide both text and image:</p>
            <ol>
                <li>Upload or capture a facial image</li>
                <li>Enter text describing your feelings</li>
                <li>Click "Analyze Emotion"</li>
            </ol>

            <div class="info-box tip">
                <div class="info-box-title">üí° Tip</div>
                <p>Multi-modal analysis combines both inputs for 95%+ accuracy and provides 
                more nuanced insights into emotional state.</p>
            </div>

            <h3>Understanding Results</h3>
            <p>The system provides:</p>
            <ul>
                <li><strong>Primary Emotion:</strong> The dominant detected emotion</li>
                <li><strong>Confidence Score:</strong> Accuracy percentage (0-100%)</li>
                <li><strong>Emotion Breakdown:</strong> Scores for all emotions</li>
                <li><strong>Demographics:</strong> Age and gender estimation (facial only)</li>
                <li><strong>Task Recommendations:</strong> Context-aware suggestions</li>
                <li><strong>Priority Level:</strong> High, Medium, or Low</li>
                <li><strong>HR Alert:</strong> If negative emotion detected</li>
            </ul>
        </section>

        <!-- API Reference -->
        <section class="doc-section" id="api">
            <h2>üîå API Reference</h2>

            <h3>POST /analyze</h3>
            <p>Main endpoint for multi-modal emotion analysis</p>

            <h4>Request Parameters:</h4>
            <table class="doc-table">
                <thead>
                    <tr>
                        <th>Parameter</th>
                        <th>Type</th>
                        <th>Required</th>
                        <th>Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td>user_id</td>
                        <td>string</td>
                        <td>Optional</td>
                        <td>User identifier (default: "anonymous")</td>
                    </tr>
                    <tr>
                        <td>image</td>
                        <td>file</td>
                        <td>Optional*</td>
                        <td>Face image file (JPG/PNG)</td>
                    </tr>
                    <tr>
                        <td>image_data</td>
                        <td>string</td>
                        <td>Optional*</td>
                        <td>Base64 encoded image</td>
                    </tr>
                    <tr>
                        <td>text</td>
                        <td>string</td>
                        <td>Optional*</td>
                        <td>Text to analyze</td>
                    </tr>
                </tbody>
            </table>
            <p><em>* At least one of image, image_data, or text must be provided</em></p>

            <h4>Example Request:</h4>
            <div class="code-header">cURL</div>
            <div class="code-block">
                <code>curl -X POST http://localhost:5000/analyze \
  -F "user_id=user_001" \
  -F "image=@face.jpg" \
  -F "text=I am feeling great today!"</code>
            </div>

            <h4>Example Response:</h4>
            <div class="code-header">JSON</div>
            <div class="code-block">
                <code>{
  "success": true,
  "final_emotion": "happy",
  "final_confidence": 94.5,
  "modality": "multimodal",
  "facial_analysis": {
    "emotion": "happy",
    "confidence": 96.2,
    "age": 28,
    "gender": "Woman",
    "all_emotions": {
      "happy": 96.2,
      "neutral": 2.1,
      "surprise": 1.2,
      "sad": 0.3,
      "angry": 0.1,
      "fear": 0.05,
      "disgust": 0.05
    }
  },
  "text_analysis": {
    "primary_emotion": "happy",
    "confidence": 92.8,
    "detailed_emotions": [
      {"emotion": "joy", "score": 94.5},
      {"emotion": "excitement", "score": 87.3},
      {"emotion": "optimism", "score": 76.2}
    ]
  },
  "recommendations": [
    "Tackle high-priority strategic projects",
    "Lead brainstorming sessions",
    "Mentor team members"
  ],
  "priority": "low",
  "category": "productivity",
  "motivation": "You're in a great mindset!",
  "hr_alerted": false,
  "timestamp": "2024-11-03 14:30:22"
}</code>
            </div>

            <h3>GET /history/{user_id}</h3>
            <p>Retrieve emotion history for a specific user</p>

            <h4>Example:</h4>
            <div class="code-header">cURL</div>
            <div class="code-block">
                <code>curl http://localhost:5000/history/user_001</code>
            </div>

            <h3>GET /analytics/{user_id}</h3>
            <p>Get aggregated emotion analytics</p>

            <h4>Example Response:</h4>
            <div class="code-header">JSON</div>
            <div class="code-block">
                <code>{
  "success": true,
  "emotion_distribution": {
    "happy": 45,
    "neutral": 30,
    "sad": 10,
    "fear": 8,
    "angry": 4,
    "surprise": 2,
    "disgust": 1
  },
  "modality_distribution": {
    "multimodal": 60,
    "text_only": 25,
    "facial_only": 15
  },
  "total_hr_alerts": 5
}</code>
            </div>
        </section>

        <!-- Model Details -->
        <section class="doc-section" id="models">
            <h2>ü§ñ Model Details</h2>

            <h3>GoEmotions RoBERTa (Text Analysis)</h3>
            <table class="doc-table">
                <tr>
                    <td><strong>Model Name</strong></td>
                    <td>SamLowe/roberta-base-go_emotions</td>
                </tr>
                <tr>
                    <td><strong>Architecture</strong></td>
                    <td>RoBERTa (Robustly Optimized BERT)</td>
                </tr>
                <tr>
                    <td><strong>Training Data</strong></td>
                    <td>Google GoEmotions (58k Reddit comments)</td>
                </tr>
                <tr>
                    <td><strong>Emotions</strong></td>
                    <td>28 fine-grained emotions</td>
                </tr>
                <tr>
                    <td><strong>Accuracy</strong></td>
                    <td>88-92%</td>
                </tr>
                <tr>
                    <td><strong>Model Size</strong></td>
                    <td>~500MB</td>
                </tr>
            </table>

            <h3>DeepFace FER-2013 (Facial Analysis)</h3>
            <table class="doc-table">
                <tr>
                    <td><strong>Framework</strong></td>
                    <td>DeepFace</td>
                </tr>
                <tr>
                    <td><strong>Training Data</strong></td>
                    <td>FER-2013 (35,887 face images)</td>
                </tr>
                <tr>
                    <td><strong>Emotions</strong></td>
                    <td>7 basic emotions</td>
                </tr>
                <tr>
                    <td><strong>Accuracy</strong></td>
                    <td>93-95%</td>
                </tr>
                <tr>
                    <td><strong>Additional</strong></td>
                    <td>Age & gender estimation</td>
                </tr>
                <tr>
                    <td><strong>Model Size</strong></td>
                    <td>~200MB</td>
                </tr>
            </table>
        </section>

        <!-- Code Examples -->
        <section class="doc-section" id="examples">
            <h2>üíª Code Examples</h2>

            <h3>Python Example</h3>
            <div class="code-header">Python</div>
            <div class="code-block">
                <code>import requests

# Text analysis
response = requests.post(
    'http://localhost:5000/analyze',
    data={
        'user_id': 'user_001',
        'text': 'I am feeling stressed about work'
    }
)

result = response.json()
print(f"Emotion: {result['final_emotion']}")
print(f"Confidence: {result['final_confidence']}%")
print(f"Recommendations: {result['recommendations']}")</code>
            </div>

            <h3>JavaScript Example</h3>
            <div class="code-header">JavaScript</div>
            <div class="code-block">
                <code>// Multi-modal analysis
const formData = new FormData();
formData.append('user_id', 'user_001');
formData.append('image', imageFile);
formData.append('text', 'Feeling anxious');

fetch('http://localhost:5000/analyze', {
    method: 'POST',
    body: formData
})
.then(response => response.json())
.then(data => {
    console.log('Emotion:', data.final_emotion);
    console.log('Tasks:', data.recommendations);
});</code>
            </div>
        </section>

        <!-- Troubleshooting -->
        <section class="doc-section" id="troubleshooting">
            <h2>üîß Troubleshooting</h2>

            <h3>Common Issues</h3>

            <h4>1. Face Not Detected</h4>
            <div class="info-box warning">
                <p><strong>Solution:</strong> Ensure good lighting, clear frontal face, 
                and no obstructions. Try the enforce_detection=False setting in code.</p>
            </div>

            <h4>2. Slow Performance</h4>
            <div class="info-box note">
                <p><strong>Solution:</strong> Enable GPU acceleration by installing CUDA-enabled PyTorch. 
                Expected: <200ms with GPU, <2s with CPU.</p>
            </div>

            <h4>3. Model Download Fails</h4>
            <div class="info-box tip">
                <p><strong>Solution:</strong> Check internet connection. Set longer timeout or use mirror:
                <code>export HF_HUB_DOWNLOAD_TIMEOUT=300</code></p>
            </div>

            <h4>4. Out of Memory Error</h4>
            <div class="info-box warning">
                <p><strong>Solution:</strong> Reduce batch size or use CPU. Clear cache: 
                <code>torch.cuda.empty_cache()</code></p>
            </div>

            <p>For more help, visit our <a href="{{ url_for('references') }}" style="color: #667eea;">References page</a> or contact support.</p>
        </section>
    </main>
</div>
{% endblock %}

{% block extra_js %}
<script>
    // Smooth scroll for sidebar navigation
    document.querySelectorAll('.doc-nav a').forEach(link => {
        link.addEventListener('click', function(e) {
            e.preventDefault();
            
            // Update active state
            document.querySelectorAll('.doc-nav a').forEach(a => a.classList.remove('active'));
            this.classList.add('active');
            
            // Scroll to section
            const targetId = this.getAttribute('href');
            const targetElement = document.querySelector(targetId);
            if (targetElement) {
                targetElement.scrollIntoView({ behavior: 'smooth', block: 'start' });
            }
        });
    });

    // Update active link on scroll
    window.addEventListener('scroll', () => {
        const sections = document.querySelectorAll('.doc-section');
        let current = '';

        sections.forEach(section => {
            const sectionTop = section.offsetTop;
            const sectionHeight = section.clientHeight;
            if (pageYOffset >= (sectionTop - 150)) {
                current = section.getAttribute('id');
            }
        });

        document.querySelectorAll('.doc-nav a').forEach(link => {
            link.classList.remove('active');
            if (link.getAttribute('href') === `#${current}`) {
                link.classList.add('active');
            }
        });
    });
</script>
{% endblock %}